{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":3600556,"datasetId":2159104,"databundleVersionId":3653976}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import ViTForImageClassification\nfrom torch.optim import AdamW\nfrom datasets import load_dataset\nfrom torchvision import transforms\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nimport torch.nn.functional as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:35:44.593654Z","iopub.execute_input":"2025-06-09T18:35:44.594452Z","iopub.status.idle":"2025-06-09T18:35:44.598543Z","shell.execute_reply.started":"2025-06-09T18:35:44.594422Z","shell.execute_reply":"2025-06-09T18:35:44.597702Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:31:54.228187Z","iopub.execute_input":"2025-06-09T18:31:54.229122Z","iopub.status.idle":"2025-06-09T18:31:54.232975Z","shell.execute_reply.started":"2025-06-09T18:31:54.229080Z","shell.execute_reply":"2025-06-09T18:31:54.232291Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"junkal/flowerdatasets\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:31:55.388401Z","iopub.execute_input":"2025-06-09T18:31:55.388658Z","iopub.status.idle":"2025-06-09T18:32:28.288648Z","shell.execute_reply.started":"2025-06-09T18:31:55.388640Z","shell.execute_reply":"2025-06-09T18:32:28.287977Z"}},"outputs":[{"name":"stdout","text":"Mounting files to /kaggle/input/flowerdatasets...\nPath to dataset files: /kaggle/input/flowerdatasets\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"train_dir = '/kaggle/input/flowerdatasets/flowers/train'\ntest_dir = '/kaggle/input/flowerdatasets/flowers/test'\nval_dir = '/kaggle/input/flowerdatasets/flowers/val'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:32:34.974061Z","iopub.execute_input":"2025-06-09T18:32:34.974755Z","iopub.status.idle":"2025-06-09T18:32:34.978000Z","shell.execute_reply.started":"2025-06-09T18:32:34.974732Z","shell.execute_reply":"2025-06-09T18:32:34.977343Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_dataset = datasets.ImageFolder(train_dir, transform=transform)\nval_dataset = datasets.ImageFolder(val_dir, transform=transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:33:31.608558Z","iopub.execute_input":"2025-06-09T18:33:31.609166Z","iopub.status.idle":"2025-06-09T18:33:45.241791Z","shell.execute_reply.started":"2025-06-09T18:33:31.609142Z","shell.execute_reply":"2025-06-09T18:33:45.241244Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:33:48.383691Z","iopub.execute_input":"2025-06-09T18:33:48.383939Z","iopub.status.idle":"2025-06-09T18:33:48.388368Z","shell.execute_reply.started":"2025-06-09T18:33:48.383921Z","shell.execute_reply":"2025-06-09T18:33:48.387626Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"num_classes = len(train_dataset.classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:33:50.473591Z","iopub.execute_input":"2025-06-09T18:33:50.473867Z","iopub.status.idle":"2025-06-09T18:33:50.477651Z","shell.execute_reply.started":"2025-06-09T18:33:50.473846Z","shell.execute_reply":"2025-06-09T18:33:50.476964Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:34:30.209717Z","iopub.execute_input":"2025-06-09T18:34:30.210250Z","iopub.status.idle":"2025-06-09T18:34:30.213785Z","shell.execute_reply.started":"2025-06-09T18:34:30.210228Z","shell.execute_reply":"2025-06-09T18:34:30.213064Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"model = ViTForImageClassification.from_pretrained(\n    'google/vit-base-patch16-224-in21k',\n    num_labels=num_classes\n).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:34:32.448052Z","iopub.execute_input":"2025-06-09T18:34:32.448764Z","iopub.status.idle":"2025-06-09T18:34:32.990765Z","shell.execute_reply.started":"2025-06-09T18:34:32.448741Z","shell.execute_reply":"2025-06-09T18:34:32.990237Z"}},"outputs":[{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=3e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:34:39.529653Z","iopub.execute_input":"2025-06-09T18:34:39.529913Z","iopub.status.idle":"2025-06-09T18:34:39.534503Z","shell.execute_reply.started":"2025-06-09T18:34:39.529895Z","shell.execute_reply":"2025-06-09T18:34:39.533783Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"num_epochs = 3\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    total_correct = 0\n    for batch in train_loader:\n        images, labels = batch[0].to(device), batch[1].to(device)\n        outputs = model(images).logits\n        loss = F.cross_entropy(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        total_loss += loss.item()\n        preds = outputs.argmax(dim=1)\n        total_correct += (preds == labels).sum().item()\n    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss/len(train_loader):.4f} - Accuracy: {total_correct/len(train_dataset):.4f}\")\n\n# Optional: validation\nmodel.eval()\nval_correct = 0\nwith torch.no_grad():\n    for batch in val_loader:\n        images, labels = batch[0].to(device), batch[1].to(device)\n        outputs = model(images).logits\n        preds = outputs.argmax(dim=1)\n        val_correct += (preds == labels).sum().item()\nprint(f\"Validation Accuracy: {val_correct/len(val_dataset):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:35:54.405315Z","iopub.execute_input":"2025-06-09T18:35:54.405558Z","iopub.status.idle":"2025-06-09T18:50:33.964844Z","shell.execute_reply.started":"2025-06-09T18:35:54.405540Z","shell.execute_reply":"2025-06-09T18:50:33.963995Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/3 - Loss: 0.4646 - Accuracy: 0.9751\nEpoch 2/3 - Loss: 0.0918 - Accuracy: 0.9994\nEpoch 3/3 - Loss: 0.0505 - Accuracy: 1.0000\nValidation Accuracy: 0.9991\n","output_type":"stream"}],"execution_count":21}]}